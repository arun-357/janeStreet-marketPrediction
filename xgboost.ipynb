{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I would like to thank @code1110. This notebook is inspired from him his work!**\n",
    "\n",
    "https://www.kaggle.com/code1110/janestreet-faster-inference-by-xgb-with-treelite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This section installs the Treelite library, which is used for model compilation and deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip --quiet install ../input/treelite/treelite-0.93-py3-none-manylinux2010_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip --quiet install ../input/treelite/treelite_runtime-0.93-py3-none-manylinux2010_x86_64.whl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os, sys\n",
    "import gc\n",
    "import math\n",
    "import random\n",
    "import pathlib\n",
    "from tqdm import tqdm\n",
    "from typing import List, NoReturn, Union, Tuple, Optional, Text, Generic, Callable, Dict\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, QuantileTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import linear_model\n",
    "import operator\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "# treelite\n",
    "import treelite\n",
    "import treelite_runtime \n",
    "\n",
    "sns.set_context(\"talk\")\n",
    "style.use('fivethirtyeight')\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PurgedGroupTimeSeriesSplit\n",
    "\n",
    "The `PurgedGroupTimeSeriesSplit` is a custom cross-validation splitter designed for time series data where observations are grouped and there is a need to avoid data leakage. This splitter is particularly useful in financial time series data where the order of data points is crucial, and leakage from future data points can lead to overly optimistic performance estimates.\n",
    "##### Key Features:\n",
    "- **Group Awareness**: Ensures that the splits respect the grouping of data points, which is essential when the data points within a group are not independent.\n",
    "- **Time Series Purging**: Implements a purging mechanism to remove data points from the training set that are too close to the test set, thus preventing leakage of information from the future into the past.\n",
    "- **Customizable Parameters**: Allows customization of the number of splits, the size of the test set, and the purging window.\n",
    "##### Usage:\n",
    "This splitter can be used with scikit-learn's cross-validation tools, such as `cross_val_score` or `GridSearchCV`, to ensure that the model evaluation is robust and free from data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false,
    "_kg_hide-output": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection._split import _BaseKFold, indexable, _num_samples\n",
    "from sklearn.utils.validation import _deprecate_positional_args\n",
    "class PurgedGroupTimeSeriesSplit(_BaseKFold):\n",
    "\n",
    "    @_deprecate_positional_args\n",
    "    def __init__(self,\n",
    "                 n_splits=5,\n",
    "                 *,\n",
    "                 max_train_group_size=np.inf,\n",
    "                 max_test_group_size=np.inf,\n",
    "                 group_gap=None,\n",
    "                 verbose=False\n",
    "                 ):\n",
    "        super().__init__(n_splits, shuffle=False, random_state=None)\n",
    "        self.max_train_group_size = max_train_group_size\n",
    "        self.group_gap = group_gap\n",
    "        self.max_test_group_size = max_test_group_size\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        \n",
    "        if groups is None:\n",
    "            raise ValueError(\n",
    "                \"The 'groups' parameter should not be None\")\n",
    "        X, y, groups = indexable(X, y, groups)\n",
    "        n_samples = _num_samples(X)\n",
    "        n_splits = self.n_splits\n",
    "        group_gap = self.group_gap\n",
    "        max_test_group_size = self.max_test_group_size\n",
    "        max_train_group_size = self.max_train_group_size\n",
    "        n_folds = n_splits + 1\n",
    "        group_dict = {}\n",
    "        u, ind = np.unique(groups, return_index=True)\n",
    "        unique_groups = u[np.argsort(ind)]\n",
    "        n_samples = _num_samples(X)\n",
    "        n_groups = _num_samples(unique_groups)\n",
    "        for idx in np.arange(n_samples):\n",
    "            if (groups[idx] in group_dict):\n",
    "                group_dict[groups[idx]].append(idx)\n",
    "            else:\n",
    "                group_dict[groups[idx]] = [idx]\n",
    "        if n_folds > n_groups:\n",
    "            raise ValueError(\n",
    "                (\"Cannot have number of folds={0} greater than\"\n",
    "                 \" the number of groups={1}\").format(n_folds,\n",
    "                                                     n_groups))\n",
    "\n",
    "        group_test_size = min(n_groups // n_folds, max_test_group_size)\n",
    "        group_test_starts = range(n_groups - n_splits * group_test_size,\n",
    "                                  n_groups, group_test_size)\n",
    "        for group_test_start in group_test_starts:\n",
    "            train_array = []\n",
    "            test_array = []\n",
    "\n",
    "            group_st = max(0, group_test_start - group_gap - max_train_group_size)\n",
    "            for train_group_idx in unique_groups[group_st:(group_test_start - group_gap)]:\n",
    "                train_array_tmp = group_dict[train_group_idx]\n",
    "                \n",
    "                train_array = np.sort(np.unique(\n",
    "                                      np.concatenate((train_array,\n",
    "                                                      train_array_tmp)),\n",
    "                                      axis=None), axis=None)\n",
    "\n",
    "            train_end = train_array.size\n",
    " \n",
    "            for test_group_idx in unique_groups[group_test_start:\n",
    "                                                group_test_start +\n",
    "                                                group_test_size]:\n",
    "                test_array_tmp = group_dict[test_group_idx]\n",
    "                test_array = np.sort(np.unique(\n",
    "                                              np.concatenate((test_array,\n",
    "                                                              test_array_tmp)),\n",
    "                                     axis=None), axis=None)\n",
    "\n",
    "            test_array  = test_array[group_gap:]\n",
    "            \n",
    "            \n",
    "            if self.verbose > 0:\n",
    "                    pass\n",
    "                    \n",
    "            yield [int(i) for i in train_array], [int(i) for i in test_array]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Denoising the Target feature\n",
    "\n",
    "In this section, we will apply a denoising technique to the target feature. Denoising helps in removing noise from the data, which can improve the performance of our model. One common method for denoising is using a moving average filter.\n",
    "\n",
    "#### Steps to Denoise the Target Feature\n",
    "\n",
    "1. **Load the necessary libraries**: We will use pandas and numpy for data manipulation.\n",
    "2. **Calculate the moving average**: We will use a rolling window to calculate the moving average of the target feature.\n",
    "3. **Replace the original target with the denoised target**: This will help in reducing the noise in the target feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# Snippets from Marco Lopez de Prado, 2020\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "def mpPDF(var,q,pts):\n",
    "    # Marcenko-Pastur pdf\n",
    "    # q=T/N\n",
    "    eMin, eMax = var*(1-(1./q)**.5)**2, var*(1+(1./q)**.5)**2\n",
    "    eVal = np.linspace(eMin,eMax,pts)\n",
    "    pdf = q/(2*np.pi*var*eVal)*((eMax-eVal)*(eVal-eMin))**.5\n",
    "    pdf = pd.Series(pdf.reshape(-1,), index=eVal.reshape(-1,))\n",
    "    return pdf\n",
    "\n",
    "\n",
    "def getPCA(matrix):\n",
    "    # Get eVal,eVec from a Hermitian matrix\n",
    "    eVal,eVec = np.linalg.eigh(matrix)\n",
    "    indices=eVal.argsort()[::-1] # arguments for sorting eVal desc\n",
    "    eVal,eVec=eVal[indices],eVec[:,indices]\n",
    "    eVal=np.diagflat(eVal)\n",
    "    return eVal,eVec\n",
    "\n",
    "def fitKDE(obs,bWidth=.25,kernel='gaussian',x=None):\n",
    "    # Fit kernel to a series of obs, and derive the prob of obs\n",
    "    # x is the array of values on which the fit KDE will be evaluated\n",
    "    if len(obs.shape)==1:\n",
    "        obs=obs.reshape(-1,1)\n",
    "    kde=KernelDensity(kernel=kernel,bandwidth=bWidth).fit(obs)\n",
    "    if x is None:\n",
    "        x=np.unique(obs).reshape(-1,)\n",
    "    if len(x.shape)==1:\n",
    "        x=x.reshape(-1,1)\n",
    "    logProb=kde.score_samples(x) # log(density)\n",
    "    pdf=pd.Series(np.exp(logProb),index=x.flatten())\n",
    "    return pdf\n",
    "\n",
    "def cov2corr(cov):\n",
    "    # Derive the correlation matrix from a covariance matrix\n",
    "    std=np.sqrt(np.diag(cov))\n",
    "    corr=cov/np.outer(std,std)\n",
    "    corr[corr<-1],corr[corr>1]=-1,1 # numerical error\n",
    "    return corr\n",
    "\n",
    "def errPDFs(var,eVal,q,bWidth,pts=1000):\n",
    "    # Fit error\n",
    "    pdf0=mpPDF(var,q,pts) # theoretical pdf\n",
    "    pdf1=fitKDE(eVal,bWidth,x=pdf0.index.values) # empirical pdf\n",
    "    sse=np.sum((pdf1-pdf0)**2)\n",
    "    return sse\n",
    "\n",
    "def findMaxEval(eVal,q,bWidth):\n",
    "    # Find max random eVal by fitting Marcenko’s dist\n",
    "    out=minimize(lambda *x:errPDFs(*x),.5,args=(eVal,q,bWidth),\n",
    "    bounds=((1E-5,1-1E-5),))\n",
    "    if out['success']:\n",
    "        var=out['x'][0]\n",
    "    else:\n",
    "        var=1\n",
    "    eMax=var*(1+(1./q)**.5)**2\n",
    "    return eMax,var\n",
    "\n",
    "def denoisedCorr(eVal,eVec,nFacts):\n",
    "    # Remove noise from corr by fixing random eigenvalues\n",
    "    eVal_=np.diag(eVal).copy()\n",
    "    eVal_[nFacts:]=eVal_[nFacts:].sum()/float(eVal_.shape[0] - nFacts)\n",
    "    eVal_=np.diag(eVal_)\n",
    "    corr1=np.dot(eVec,eVal_).dot(eVec.T)\n",
    "    corr1=cov2corr(corr1)\n",
    "    return corr1\n",
    "\n",
    "def denoisedCorr2(eVal,eVec,nFacts,alpha=0):\n",
    "    # Remove noise from corr through targeted shrinkage\n",
    "    eValL,eVecL=eVal[:nFacts,:nFacts],eVec[:,:nFacts]\n",
    "    eValR,eVecR=eVal[nFacts:,nFacts:],eVec[:,nFacts:]\n",
    "    corr0=np.dot(eVecL,eValL).dot(eVecL.T)\n",
    "    corr1=np.dot(eVecR,eValR).dot(eVecR.T)\n",
    "    corr2=corr0+alpha*corr1+(1-alpha)*np.diag(np.diag(corr1))\n",
    "    return corr2\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "#@njit\n",
    "def fillna_npwhere_njit(array, values):\n",
    "    if np.isnan(array.sum()):\n",
    "        array = np.where(np.isnan(array), values, array)\n",
    "    return array\n",
    "\n",
    "class RMTDenoising(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, bWidth=.01, alpha=.5, feature_0=True, sample=0.3, seed=2021):\n",
    "        self.bWidth = bWidth\n",
    "        self.alpha = alpha\n",
    "        self.feature_0 = feature_0\n",
    "        self.sample = sample\n",
    "        self.seed = seed\n",
    "    \n",
    "    def denoise(self, X):\n",
    "        sample = X.sample(frac=self.sample, random_state=self.seed)\n",
    "        q = X.shape[0] / X.shape[1]\n",
    "        cov = sample.cov().values\n",
    "        corr0 = cov2corr(cov)\n",
    "\n",
    "        eVal0, eVec0 = getPCA(corr0)\n",
    "        eMax0, var0 = findMaxEval(np.diag(eVal0), q, bWidth=self.bWidth)\n",
    "        nFacts0 = eVal0.shape[0] - np.diag(eVal0)[::-1].searchsorted(eMax0)\n",
    "        corr1 = denoisedCorr2(eVal0,eVec0,nFacts0,alpha=self.alpha)\n",
    "        eVal1, eVec1 = getPCA(corr1)\n",
    "        #result = np.hstack((np.diag(eVal1), var0))\n",
    "        #name = [f'eigen_{i+1}' for i in range(len(eVal1))] + ['var_explained']\n",
    "        return eVec1[:, :nFacts0]\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        if self.feature_0:\n",
    "            self.cols_ = [c for c in X.columns if c != 'feature_0']\n",
    "        else:\n",
    "            self.cols_ = list(X.columns)\n",
    "        X_ = X[self.cols_]\n",
    "        self.W_ = self.denoise(X_)\n",
    "        self.dim_W_ = self.W_.shape[1]\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X_ = X.copy()\n",
    "        names = [f'proj_{i}' for i in range(self.dim_W_)]\n",
    "        projection = pd.DataFrame(fillna_npwhere_njit(X_[self.cols_].values, 0).dot(self.W_), columns=names)\n",
    "        if self.feature_0:\n",
    "            projection['feature_0'] = X['feature_0']\n",
    "        return projection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "SEED = 42 \n",
    "# INPUT_DIR = '../input/jane-street-market-prediction/'\n",
    "START_DATE = 85\n",
    "INPUT_DIR = '../input/janestreet-save-as-feather/'\n",
    "TRADING_THRESHOLD = 0.502 \n",
    "os.listdir(INPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def load_data(input_dir=INPUT_DIR):\n",
    "    train = pd.read_feather(pathlib.Path(input_dir + 'train.feather'))\n",
    "    #features = pd.read_feather(pathlib.Path(input_dir + 'features.feather'))\n",
    "    #example_test = pd.read_feather(pathlib.Path(input_dir + 'example_test.feather'))\n",
    "    #ss = pd.read_feather(pathlib.Path(input_dir + 'example_sample_submission.feather'))\n",
    "    return train\n",
    "train = load_data(INPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation of Columns Removed\n",
    "\n",
    "In this section, we discuss the columns that were removed from the dataset. The removal was based on two main criteria:\n",
    "\n",
    "1. **Feature Selection**: This involves selecting the most important features that contribute to the predictive power of the model. Irrelevant or less important features are removed to improve model performance and reduce complexity.\n",
    "\n",
    "2. **Collinearity**: Collinearity refers to a situation where two or more predictor variables are highly correlated. High collinearity can cause issues in the model, such as inflated standard errors and less reliable estimates. To address this, we remove one of the correlated variables to ensure the model remains robust and interpretable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "drop = {'feature_100', 'feature_101', 'feature_102', 'feature_107', 'feature_108', 'feature_110','feature_112', 'feature_113', 'feature_114', 'feature_116',\n",
    " 'feature_119', 'feature_12', 'feature_122', 'feature_124', 'feature_125', 'feature_126', 'feature_127', 'feature_128', 'feature_129', 'feature_14',\n",
    " 'feature_18', 'feature_20', 'feature_21', 'feature_22', 'feature_23', 'feature_24', 'feature_25', 'feature_26', 'feature_28', 'feature_30', 'feature_31',\n",
    " 'feature_32', 'feature_33', 'feature_34', 'feature_35', 'feature_36', 'feature_38', 'feature_4', 'feature_40', 'feature_47', 'feature_48',\n",
    " 'feature_49', 'feature_51', 'feature_57', 'feature_58', 'feature_6', 'feature_61', 'feature_63', 'feature_66', 'feature_68', 'feature_69', 'feature_71', 'feature_76',\n",
    " 'feature_8', 'feature_88', 'feature_96'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preprocessing\n",
    "\n",
    "In this section, we preprocess the data to prepare it for modeling. The preprocessing steps include:\n",
    "\n",
    "1. **Filtering the Data**: We filter the training data to include only the dates after the specified `START_DATE` and remove rows where the weight is zero.\n",
    "2. **Handling Missing Values**: We fill missing values in the dataset with the mean of the respective columns.\n",
    "3. **Dropping Unnecessary Features**: We drop the features that were identified as less important or highly collinear in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train = train.query(f'date > {START_DATE}')\n",
    "train.fillna(train.mean(),inplace=True)\n",
    "train = train[train['weight'] != 0]\n",
    "train = train.drop(drop,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Engineering\n",
    "\n",
    "In this section, we perform feature engineering to create new features that can help improve the performance of our model. Feature engineering involves transforming raw data into meaningful features that better represent the underlying patterns in the data.\n",
    "\n",
    "##### Steps for Feature Engineering:\n",
    "\n",
    "1. **Denoising the Target Feature**: We apply a denoising technique to the target feature to remove noise and improve the signal quality.\n",
    "2. **Creating New Features**: We create new features based on the existing ones to capture additional information that may be useful for the model.\n",
    "3. **Feature Selection**: We select the most important features that contribute to the predictive power of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "targets = ['resp','resp_1','resp_2','resp_3','resp_4']\n",
    "\n",
    "targets_f0 = targets + ['feature_0']\n",
    "target_tf = RMTDenoising(sample=0.8)\n",
    "\n",
    "target_tf.fit(train[targets_f0])\n",
    "\n",
    "Dn_targets = target_tf.transform(train[targets_f0])\n",
    "\n",
    "train['dresp'] = -Dn_targets.proj_0\n",
    "\n",
    "#train['action'] =  (  (train['resp_1'] > 0 ) & (train['resp_2'] > 0 ) & (train['resp_3'] > 0 ) & (train['resp_4'] > 0 ) &  (train['resp'] > 0 ) &  (train['dresp'] > 0 )   ).astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "features = train.columns[train.columns.str.startswith('feature')].values.tolist()\n",
    "print('{} features used'.format(len(features)))\n",
    "train['action'] = (train['resp'] > 0).astype('int')\n",
    "f_mean = np.mean(train[features[1:]].values,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This block of code contains the best training parameters for the XGBoost model, which were found using Optuna, a hyperparameter optimization framework. The optimization process was conducted in a Google Colab notebook, leveraging its computational resources to efficiently search for the optimal parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "params = {'n_estimators': 658, 'max_depth': 5, 'min_child_weight': 5, 'learning_rate': 0.04067234688762774,\n",
    " 'subsample': 0.8550706590979082, 'gamma': 0, 'colsample_bytree': 0.976077957177498,\n",
    "'objective':'binary:logistic','eval_metric': 'auc','tree_method': 'gpu_hist',  \n",
    "        'random_state': 42,} # best 5600\n",
    "params_1 = {'n_estimators': 819, 'max_depth': 6, 'min_child_weight': 3, 'learning_rate':0.02394470312114685,\n",
    " 'subsample':  0.7303852815955237, 'gamma': 0, 'colsample_bytree':  0.770496590789439,\n",
    "'objective':'binary:logistic','eval_metric': 'auc','tree_method': 'gpu_hist',  \n",
    "        'random_state': 42,} # 5000\n",
    "\n",
    "params_2 = {'n_estimators': 752, 'max_depth': 11, 'min_child_weight': 4, 'learning_rate':0.03586904895157962,\n",
    " 'subsample':  0.6299777543887805, 'gamma': 0, 'colsample_bytree':  0.8066226408240955,\n",
    "'objective':'binary:logistic','eval_metric': 'auc','tree_method': 'gpu_hist',  \n",
    "        'random_state': 42,} # 3000\n",
    "\n",
    "params_3 = {'n_estimators': 766, 'max_depth': 10, 'min_child_weight': 7, 'learning_rate': 0.010029649857344392,\n",
    " 'subsample':  0.8377963223123193, 'gamma': 0, 'colsample_bytree':  0.965897457878852,\n",
    "'objective':'binary:logistic','eval_metric': 'auc','tree_method': 'gpu_hist',  \n",
    "        'random_state': 42,}\n",
    "\n",
    "params_4 =  {'n_estimators': 991, 'max_depth': 7, 'min_child_weight': 2,\n",
    "  'learning_rate': 0.021441916265932923, 'subsample': 0.6508037151907303, \n",
    "  'gamma': 0, 'colsample_bytree': 0.8545754382982661,'objective':'binary:logistic',\n",
    "'eval_metric': 'auc','tree_method': 'gpu_hist', 'random_state': 42,}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multitarget Classification with Time Series Data\n",
    "\n",
    "This notebook demonstrates a multitarget classification approach using **XGBoost** on time-series data. The workflow includes preprocessing, training with a custom cross-validation strategy, and saving the trained models. Additionally, it supports an inference mode for efficient predictions.\n",
    "\n",
    "---\n",
    "\n",
    "## Objective\n",
    "- Train a classifier for multiple target variables (`resp_1`, `resp_2`, `resp_3`, `resp`, and `resp_4`).\n",
    "- Ensure robust evaluation using time-series-aware cross-validation to prevent data leakage.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Steps\n",
    "\n",
    "### 1. Data Preparation\n",
    "- Extract features (`X`) and binary labels (`y`) for all target variables.\n",
    "- Group data by the `date` column for time-series handling.\n",
    "\n",
    "### 2. Cross-Validation\n",
    "- Use `PurgedGroupTimeSeriesSplit` to split data into train and validation sets.\n",
    "- This ensures a \"gap\" between splits, reducing the risk of data leakage across time steps.\n",
    "\n",
    "### 3. Model Training\n",
    "- Train separate **XGBoost classifiers** for each target variable.\n",
    "- Evaluate performance using the **ROC-AUC score** for each fold.\n",
    "- Save the best model for each target and fold.\n",
    "\n",
    "### 4. Garbage Collection\n",
    "- Use Python’s garbage collector (`gc`) to free memory during training.\n",
    "\n",
    "### 5. Inference (Optional)\n",
    "- Use precompiled models loaded with **Treelite** for faster predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "training = True\n",
    "if training:\n",
    "    import time\n",
    "    import gc\n",
    "    resp_cols = ['resp_1', 'resp_2', 'resp_3', 'resp', 'resp_4']\n",
    "    X = train[features].values\n",
    "    #y = train['action'].values\n",
    "    y = np.stack([(train[c] > 0).astype('int') for c in resp_cols]).T #Multitarget\n",
    "    groups = train['date'].values\n",
    "    models = []\n",
    "    scores = []\n",
    "\n",
    "    cv = PurgedGroupTimeSeriesSplit(\n",
    "        n_splits=4,\n",
    "        group_gap=20,\n",
    "    )\n",
    "    for t in tqdm(range(y.shape[1])):\n",
    "        yy = y[:,t]\n",
    "        for i, (train_index, valid_index) in enumerate(cv.split(\n",
    "                X,\n",
    "                yy,\n",
    "                groups=groups)):\n",
    "            print(f'Target {t} Fold {i} started at {time.ctime()}')\n",
    "            X_train, X_valid = X[train_index], X[valid_index]\n",
    "            y_train, y_valid = yy[train_index], yy[valid_index]\n",
    "            model = xgb.XGBClassifier(**params_4, n_jobs = -1)\n",
    "            model.fit(X_train, y_train, \n",
    "                    eval_set=[(X_valid, y_valid)], eval_metric='auc',\n",
    "                    verbose=100, callbacks = [xgb.callback.EarlyStopping(rounds=300,save_best=True)])\n",
    "            pred = model.predict(X_valid)\n",
    "            score = roc_auc_score(y_valid,pred)\n",
    "            model.save_model(f'my_model_{t}_{i}.model')\n",
    "            models.append(model)\n",
    "            scores.append(score)\n",
    "            del score, model\n",
    "        print(scores)\n",
    "        del X_train, X_valid, y_train, y_valid\n",
    "        rubbish = gc.collect()\n",
    "\n",
    "else:\n",
    "    predictor_0 = treelite_runtime.Predictor('../input/xgbtreelite000/mymodel_0.so', verbose=True)\n",
    "    predictor_1 = treelite_runtime.Predictor('../input/xgbtreelite000/mymodel_1.so', verbose=True)\n",
    "    predictor_2 = treelite_runtime.Predictor('../input/xgbtreelite000/mymodel_2.so', verbose=True)\n",
    "    predictor_3 = treelite_runtime.Predictor('../input/xgbtreelite000/mymodel_3.so', verbose=True)\n",
    "    predictor_4 = treelite_runtime.Predictor('../input/xgbtreelite000/mymodel_4.so', verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile with Treelite\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# pass to treelite\n",
    "if training:\n",
    "    model_0 = treelite.Model.load('my_model_0_3.model', model_format='xgboost')\n",
    "    model_1 = treelite.Model.load('my_model_1_3.model', model_format='xgboost')\n",
    "    model_2 = treelite.Model.load('my_model_2_3.model', model_format='xgboost')\n",
    "    model_3 = treelite.Model.load('my_model_3_3.model', model_format='xgboost')\n",
    "    model_4 = treelite.Model.load('my_model_4_3.model', model_format='xgboost')\n",
    "    #model_5 = treelite.Model.load('my_model_5_3.model', model_format='xgboost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "if training:\n",
    "    m = [model_0,model_1,model_2,model_3,model_4]\n",
    "    for j,i in enumerate(m):\n",
    "        toolchain = 'gcc'\n",
    "        i.export_lib(toolchain=toolchain, libpath=f'./mymodel_{j}.so',\n",
    "                     params={'parallel_comp': 32}, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if training:\n",
    "    predictor_0 = treelite_runtime.Predictor(f'./mymodel_{0}.so', verbose=True)\n",
    "    predictor_1 = treelite_runtime.Predictor(f'./mymodel_{1}.so', verbose=True)\n",
    "    predictor_2 = treelite_runtime.Predictor(f'./mymodel_{2}.so', verbose=True)\n",
    "    predictor_3 = treelite_runtime.Predictor(f'./mymodel_{3}.so', verbose=True)\n",
    "    predictor_4 = treelite_runtime.Predictor(f'./mymodel_{4}.so', verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import janestreet\n",
    "env = janestreet.make_env() # initialize the environment\n",
    "iter_test = env.iter_test() # an iterator which loops over the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "f = np.median \n",
    "index_features = [n for n in range(1,(len(features) + 1))]\n",
    "for (test_df, pred_df) in tqdm(iter_test):\n",
    "    \n",
    "    if test_df['weight'].item() > 0:\n",
    "        \n",
    "        test_df = test_df.drop(drop,axis=1)\n",
    "        x_tt = test_df.values[0][index_features].reshape(1,-1)\n",
    "            \n",
    "        if np.isnan(x_tt[:, 1:].sum()):\n",
    "            x_tt[:, 1:] = np.nan_to_num(x_tt[:, 1:]) + np.isnan(x_tt[:, 1:]) * f_mean\n",
    "        \n",
    "        # inference with treelite\n",
    "        batch = treelite_runtime.Batch.from_npy2d(x_tt)\n",
    "        pred_0 = predictor_0.predict(batch)\n",
    "        pred_1 = predictor_1.predict(batch)\n",
    "        pred_2 = predictor_2.predict(batch)\n",
    "        pred_3 = predictor_3.predict(batch)\n",
    "        pred_4 = predictor_4.predict(batch)\n",
    "        \n",
    "        # Version 9\n",
    "        pred = np.stack([pred_0,pred_1,pred_2,pred_3,pred_4],axis=0).T\n",
    "        pred = f(pred)\n",
    "        pred_df.action = int(pred >= TRADING_THRESHOLD)\n",
    "\n",
    "    else:\n",
    "         pred_df['action'].values[0] = 0\n",
    "    env.predict(pred_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
